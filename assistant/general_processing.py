from transformers import pipeline
from assistant.response_generation import speak


# Load the model and tokenizer
# You can replace "gpt2" with other Hugging Face models
nlp_pipeline = pipeline("text-generation", model="gpt2", framework="pt") 

def process_with_local_nlp(query):
    """
    Handles general queries using a Hugging Face Transformers model.

    Args:
        query (str): The user's input query.

    Returns:
        str: The response generated by the NLP model.
    """
    try:
        response = nlp_pipeline(query, max_length=100, num_return_sequences=1)
        return response[0]["generated_text"]
    except Exception as e:
        speak("Sorry, I couldn't process the query.")
        print(f"Sorry, I couldn't process the query due to: {str(e)}")
