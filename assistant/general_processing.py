import logging
from transformers import pipeline
from assistant.response_generation import speak


logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

nlp_pipeline = pipeline("text-generation", model="gpt2", framework="pt")


def process_with_local_nlp(query: str) -> str:
    """
    Processes user queries using a Hugging Face Transformers model to generate a response.

    Args:
        query (str): The user's input query.

    Returns:
        str: The response generated by the NLP model or a fallback message in case of failure.
    """
    try:
        logger.info("Received query: %s", query)

        response = nlp_pipeline(query, max_length=100, num_return_sequences=1)
        logger.info("Response generated successfully.")

        # Extract and return the generated text from the model's response
        return response[0]["generated_text"]

    except TimeoutError:
        logger.error("Model processing timed out.")
        speak("Sorry, the model took too long to respond. Please try again later.")
        return "Sorry, the model took too long to respond. Please try again later."

    except Exception as e:
        logger.error("Error processing query: %s", str(e))
        speak("Sorry, I couldn't process the query due to an error.")
        return f"Sorry, I couldn't process the query due to an error: {str(e)}"
