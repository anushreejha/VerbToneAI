import logging
from transformers import pipeline
from assistant.response_generation import speak


# Configure logging for better tracking and debugging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Load the NLP pipeline (can be replaced with other transformer models like GPT-3)
nlp_pipeline = pipeline("text-generation", model="gpt2", framework="pt")


def process_with_local_nlp(query: str) -> str:
    """
    Processes user queries using a Hugging Face Transformers model to generate a response.

    Args:
        query (str): The user's input query.

    Returns:
        str: The response generated by the NLP model or a fallback message in case of failure.
    """
    try:
        # Log incoming query
        logger.info("Received query: %s", query)

        response = nlp_pipeline(query, max_length=100, num_return_sequences=1)
        logger.info("Response generated successfully.")

        # Extract and return the generated text from the model's response
        return response[0]["generated_text"]

    except TimeoutError:
        logger.error("Model processing timed out.")
        speak("Sorry, the model took too long to respond. Please try again later.")
        return "Sorry, the model took too long to respond. Please try again later."

    except Exception as e:
        logger.error("Error processing query: %s", str(e))
        speak("Sorry, I couldn't process the query due to an error.")
        return f"Sorry, I couldn't process the query due to an error: {str(e)}"
